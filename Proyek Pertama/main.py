# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_EerxwO1AokpxOnIXgtCCEHk0jguVNR9

Machine Learning Terapan - Rama Syailana Dewa -

## Predictive Analytics - Klasifikasi Obesitas Berdasarkan Gaya Hidup

### Deskripsi Proyek
Proyek ini berfokus pada bidang kesehatan dan machine learning untuk mengklasifikasikan tingkat obesitas berdasarkan data gaya hidup dan kesehatan. Dataset yang digunakan mencakup berbagai variabel, seperti pola makan, aktivitas fisik, riwayat keluarga, serta pengukuran fisik (misalnya, usia, tinggi badan, dan berat badan). Tujuan dari proyek ini adalah untuk mengembangkan model prediktif yang dapat mengidentifikasi individu dengan risiko obesitas tinggi secara akurat. Hasil dari model ini diharapkan dapat digunakan sebagai dasar dalam intervensi kesehatan dan pencegahan penyakit kronis yang terkait dengan obesitas. Dua atau lebih model machine learning akan diuji, dan model dengan performa terbaik akan dipilih berdasarkan evaluasi metrik seperti akurasi, precision, recall, F1 score, dan confusion matrix.

### 1. Import Library
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Import necessary libraries
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

"""### 2. Data Understanding
Data Understanding merupakan proses memahami informasi dalam data dan menentukan kualitas data tersebut.

#### 2.1 Data Loading
Data Loading merupakan tahap untuk memuat dataset yang akan digunakan agar dataset lebih mudah dipahami

*dataset yang digunakan dalam proyek ini:*

https://www.kaggle.com/datasets/adeniranstephen/obesity-prediction-dataset
"""

df = pd.read_csv("data\ObesityDataSet_raw_and_data_sinthetic.csv")

"""**Insight**

Memuat dataset obesitas menggunakan fungsi pd.read_csv(). Data diambil dari file CSV yang berisi 2111 catatan dan 17 fitur. Pemeriksaan awal dilakukan untuk melihat beberapa baris pertama dan terakhir guna memastikan data berhasil dimuat dengan benar.

#### 2.2 Exploratory Data Analysis

Eksploratori data merupakan langkah pertama dalam menganalisa data yang bertujuan untuk mengenali ciri - ciri data, menemukan pola atau ketidakwajaran, dan menguji hipotesis yang ada. Tahap ini biasanya melibatkan penggunaan teknik statistik dan visualisasi grafik guna membantu dalam pemahaman data.

##### 2.2.1 EDA - Deskripsi Variabel
"""

df.head()

df.tail()

df.info()

df.describe()

"""#### 2.2.2 EDA - Menangani Missing Value dan Menangani Outliers"""

df.isnull().sum()

df.duplicated()

df.duplicated().sum()

df =df.drop_duplicates()

"""**Insight**

Dilakukan pemeriksaan missing values dan duplikasi. Hasil pengecekan menunjukkan bahwa tidak ada missing values, namun terdapat beberapa duplikat yang kemudian dihapus untuk menjaga keunikan data.
"""

df.columns

fig, axes = plt.subplots(1, 2, figsize=(12, 5))
sns.boxplot(y=df['Height'], ax=axes[0], color='darkblue')
sns.boxplot(y=df['Weight'], ax=axes[1], color='darkgreen')
axes[0].set_title('Height Distribution')
axes[1].set_title('Weight Distribution')

"""#### 2.2.3 EDA - Univariate Analysis"""

# 2.1 Univariate Analysis
# -------------------------
# Visualisasi distribusi variabel numerik dengan histogram
numeric_cols = df.select_dtypes(include=[np.number]).columns
plt.figure(figsize=(15, 10))
for i, col in enumerate(numeric_cols, 1):
    plt.subplot(4, 4, i)
    sns.histplot(df[col], bins=30, kde=True, color='steelblue')
    plt.title(f"Distribution of {col}")
    plt.xlabel(col)
    plt.ylabel("Frequency")
plt.tight_layout()
plt.show()

# Visualisasi count plot untuk variabel kategorikal
cat_cols = ['Gender', 'family_history_with_overweight', 'FAVC', 'CAEC',
            'SMOKE', 'SCC', 'CALC', 'MTRANS', 'NObeyesdad']
plt.figure(figsize=(15, 10))
for i, col in enumerate(cat_cols, 1):
    plt.subplot(3, 3, i)
    sns.countplot(x=col, hue=col, data=df, palette="viridis")
    plt.title(f"Count of {col}")
    plt.xlabel(col)
    plt.ylabel("Count")
plt.tight_layout()
plt.show()

"""#### 2.2.4 Multivariate Analysis"""

pd.crosstab(df['FAVC'], df['NObeyesdad']).plot(kind='bar', stacked=True, colormap='coolwarm')
plt.title('Fast Food Consumption vs. Obesity Levels')

sns.violinplot(x='FCVC', y='NObeyesdad', data=df, palette='muted')
plt.title('Vegetable Consumption vs. Obesity Levels')

sns.boxplot(x='NCP', y='Weight', data=df, hue='NObeyesdad', palette='coolwarm')
plt.title('Meal Frequency vs. Weight')

sns.stripplot(x='CALC', y='Weight', hue='NObeyesdad', data=df, palette='coolwarm', jitter=True)

sns.stripplot(x='FAF', y='Weight', hue='NObeyesdad', data=df, jitter=True, palette='coolwarm')

sns.countplot(x='MTRANS', hue='NObeyesdad', data=df, palette='Set2')

# 2.2 Multivariate Analysis
# ---------------------------
# Visualisasi pairplot untuk melihat hubungan antara variabel numerik
sns.pairplot(df[numeric_cols], diag_kind='kde', corner=True)
plt.suptitle("Pairplot for Numeric Features", y=1.02)
plt.show()

# Visualisasi Heatmap korelasi antar fitur
plt.figure(figsize=(10, 6))
sns.heatmap(df[numeric_cols].corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Feature Correlation Heatmap")
plt.show()

"""**Insight**

Analisis eksplorasi data dilakukan untuk memahami distribusi dan hubungan antar variabel. Dari histogram, terlihat sebaran nilai pada setiap variabel numerik, sedangkan count plot menunjukkan frekuensi kategori pada variabel kategorikal. Insight penting yang diperoleh antara lain: tidak ada missing value, distribusi data cukup simetris pada sebagian besar fitur, dan terdapat beberapa outlier yang mungkin memerlukan perhatian lebih lanjut.

### 3. Data Preparation
Persiapan data adalah langkah yang dilakukan untuk mengolah data agar siap digunakan dalam tahap pembuatan model Machine Learning.

#### 3.1 Encoding Variabel Kategorikal
"""

le = LabelEncoder()
df["Gender"] = le.fit_transform(df["Gender"])
df["family_history_with_overweight"] = le.fit_transform(df["family_history_with_overweight"])
df["FAVC"] = le.fit_transform(df["FAVC"])
df["CAEC"] = le.fit_transform(df["CAEC"])
df["SMOKE"] = le.fit_transform(df["SMOKE"])
df["SCC"] = le.fit_transform(df["SCC"])
df["CALC"] = le.fit_transform(df["CALC"])
df["MTRANS"] = le.fit_transform(df["MTRANS"])
df["NObeyesdad"] = le.fit_transform(df["NObeyesdad"])

"""#### 3.2 Pemisahan Fitur Target"""

# Define x and y before encoding
x = df.drop(["NObeyesdad"], axis=1)
y = df["NObeyesdad"]

# Initialize label encoder
le = LabelEncoder()

# Encode only categorical columns
for col in x.columns:
    if x[col].dtype == 'object' or x[col].dtype.name == 'category':
        x[col] = le.fit_transform(x[col])

print("Categorical encoding completed!")

print(x.dtypes)

# Create a LabelEncoder instance
le = LabelEncoder()

# Identify categorical columns (bins)
categorical_cols = [col for col in x.columns if x[col].dtype.name == 'category']

# Convert categorical columns to numeric
for col in categorical_cols:
    x[col] = le.fit_transform(x[col])

"""**Insight**

Mengubah variabel kategorikal ke format numerik menggunakan LabelEncoder

#### 3.3 Train-Test-Split
"""

# Train-test split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)

# Standardization
ss = StandardScaler()
x_train = ss.fit_transform(x_train)
x_test = ss.transform(x_test)

"""#### 3.4 Standarisasi"""

scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

print(x_train.dtype)

"""**Insight**

menstandarisasi variabel numerik dengan StandardScaler. Transformasi ini penting agar model machine learning dapat memproses semua data secara konsisten.

#### 4. Model Development
Pengembangan model adalah fase di mana algoritma machine learning diterapkan untuk menyelesaikan permasalahan yang telah diidentifikasi pada tahap pemahaman bisnis.
"""

# Define models dictionary
models = {
    "Random Forest Classifier": RandomForestClassifier(),
    "Decision Tree Classifier": DecisionTreeClassifier(),
    "Ada Boost Classifier": AdaBoostClassifier(),
    "KNN": KNeighborsClassifier(),
    "Gradient Boosting Classifier": GradientBoostingClassifier(),
    "Logistic Regression": LogisticRegression(max_iter=1000)
}

# Verifikasi dictionary model
print("Defined Models:")
print(models)

# Dictionary untuk menyimpan performa masing-masing model
results = {}

# Loop untuk training dan evaluasi setiap model
for name, model in models.items():
    # Training model
    model.fit(x_train, y_train)

    # Prediksi pada data testing
    y_pred = model.predict(x_test)

    # Menghitung metrik evaluasi
    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred, average='weighted')
    rec = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    cm = confusion_matrix(y_test, y_pred)

    # Simpan hasil evaluasi pada dictionary
    results[name] = {
       "accuracy": acc,
       "precision": prec,
       "recall": rec,
       "f1_score": f1,
       "confusion_matrix": cm
    }

    # Print hasil evaluasi untuk tiap model
    print(f"\n{name}:")
    print(f"  Accuracy: {acc:.4f}")
    print(f"  Precision: {prec:.4f}")
    print(f"  Recall: {rec:.4f}")
    print(f"  F1 Score: {f1:.4f}")
    print("  Confusion Matrix:")
    print(cm)

# Menentukan model terbaik berdasarkan akurasi (atau metrik lain sesuai kebutuhan)
best_model_name = max(results, key=lambda x: results[x]["accuracy"])
print(f"\nBest Model by Accuracy: {best_model_name}")

"""**Insight**

Berbagai model dikembangkan untuk mengklasifikasikan tingkat obesitas. Model-model yang digunakan antara lain Random Forest, Decision Tree, AdaBoost, KNN, Gradient Boosting, dan Logistic Regression. Evaluasi dilakukan dengan menghitung metrik seperti accuracy, precision, recall, F1 score, dan confusion matrix. Model Gradient Boosting menunjukkan performa terbaik dengan akurasi tertinggi, yang mendukung keandalan model dalam membantu identifikasi individu berisiko tinggi.
"""

# Visualisasi Confusion Matrix untuk model terbaik
best_cm = results[best_model_name]["confusion_matrix"]

plt.figure(figsize=(8, 6))
sns.heatmap(best_cm, annot=True, fmt='d', cmap='Blues')
plt.title(f"Confusion Matrix for {best_model_name}")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

"""**Insight**

Hasil evaluasi model divisualisasikan menggunakan heatmap untuk confusion matrix dan bar chart untuk perbandingan akurasi antar model. Visualisasi ini memudahkan analisis perbandingan kinerja setiap model, sehingga model dengan performa terbaik dapat diidentifikasi secara objektif.

**Conclusion**
Evaluasi model dilakukan untuk menentukan model yang paling sesuai dengan tujuan bisnis. Hasil evaluasi menunjukkan bahwa:

- Gradient Boosting Classifier memiliki akurasi tertinggi (sekitar 96.55%) serta metrik evaluasi lain yang sangat baik, menjadikannya kandidat terbaik untuk prediksi tingkat obesitas.
- Model lain seperti Random Forest dan Decision Tree juga menunjukkan performa yang baik, namun sedikit di bawah model Gradient Boosting.
- Ada Boost Classifier dan KNN menunjukkan performa yang lebih rendah dan tidak konsisten.

Hasil evaluasi mendukung tujuan bisnis untuk mengidentifikasi individu berisiko tinggi dan mendukung pengambilan keputusan berbasis data. Proyek ini memberikan dasar untuk mengembangkan sistem pendukung keputusan yang dapat meningkatkan efektivitas intervensi dan penanganan obesitas secara profesional.
"""